---
title: "Siamese Network with Keras(Contrastive loss, Triplet loss )"
search: True
categories: 
  - Siamese 
  - Keras
  - CNN
last_modified_at: 2020-07-19T08:06:00-05:00
toc: true
toc_label: "Index List"
toc_sticky: true
---

**2020 인공지능 경진대회를 참가하며 적용한 Siamese(샴) network를 기록하려고 합니다.**

## 대회소개 
![Unsplash image 9]({{ site.url }}{{ site.baseurl }}/assets/images/AI_competition_mainpage.PNG)

이 대회는 6월 17일~ 6월 30일까지 약 2주간 진행된 대회입니다. 
400개의 팀이 참가했고, 한 팀당 최대 5인의 팀원으로 참가가 가능합니다. 
총 20개의 문제(안면 인식, 음성, 교통 데이터...) 중 자유롭게 풀면 됩니다. 
각 문제에서 순위가 높으면 더 큰 점수를 받게되며 점수가 가장 큰 3문제의 점수를 합산하여 최종 순위를 정하게 됩니다. 

저는 안면인식과 관련된 주제를 선택했습니다. 하나의 모델로 4개의 문제에 접근이 가능하기에...
처음 안면인식쪽으로 접근해봤지만 재미있었습니다.
최종순위 60위 안에 들면 사업화 지원금 대상이 될 수도 있었지만 
67위에 머무르며 안타깝게 떨어졌습니다ㅎㅎㅎ

## 문제/데이터
제가 참가한 문제는 얼굴 다각도 인식, 액세서리 착용자 인식이었습니다. 
주어진 데이터는 Pair로 이루어져 있으며 x = (정면,측면), y = Boolean 입니다.
한 사람 당 정면 이미지 여려장, 측면 이미지 여러장으로 이루어진 few-shot learning 입니다. 

**접근 방법은 Metric Learning으로 타겟을 잡았고, Simese Network 모델을 선정했습니다.**

Siamese Network의 기본 개념은 아래 그림과 같이 CNN으로 이미지를 유클리드(Euclidean) 공간으로 나타내어 유사성을 구분하게 됩니다. 

![Unsplash image 9]({{ site.url }}{{ site.baseurl }}/assets/images/metric_learning_euclidian_ distance.PNG)
**Metric on Euclidean Space**


Siamese Network의 구조는 아래 그림과 같이 **하나의 CNN으로 Weight를 서로 공유하는** 하나의 모델로 각각의 이미지를 좌표로 나타냅니다.
유클리드 공간에서의 거리가 가까우면 유사한 이미지, 멀면 다른 이미지로 구분하는 원리입니다. 

![Unsplash image 9]({{ site.url }}{{ site.baseurl }}/assets/images/Siamese_Architecture.JPG)
**Siamese Architecture**


## 코드
Keras, Pandas, Numpy, cv2를 사용하여 전체 프로세스를 구성했습니다. 
CNN 모델은 아래처럼 구성했습니다. 
대회규칙이 Pre-trained된 model을 사용하면 안되어 간단한 구조의 CNN architecture를 구성했습니다.

```python
def EmbeddingNetwork():
    model = Sequential()
    model.add(Conv2D(32,4,input_shape = (112,112,3),strides=2,padding='same',activation='relu'))
    model.add(Conv2D(32,4,strides=2,padding='same',activation='relu'))
    model.add(MaxPool2D(pool_size=(2,2),strides=2,padding='same'))
    model.add(Conv2D(64,4,strides=1,padding='same',activation='relu'))
    model.add(Conv2D(64,4,strides=1,padding='same',activation='relu'))
    model.add(MaxPool2D(pool_size=(2,2),strides=2,padding='same'))
    model.add(Conv2D(128,2,strides=1,padding='same',activation='relu'))
    model.add(MaxPool2D(pool_size=(2,2),strides=2,padding='same'))

    model.add(Flatten(data_format=None))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation ='sigmoid'))

    return model
}
```

여기서 Loss 함수는 Contrastive Loss, Triplet Loss 함수를 생성하여 적용했습니다. 
결론적으로는 Contrastive가 정확도 80%정도 나왔고, Triplet이 92%로 정확도가 더 높았습니다.

**Contrastive Loss**
Contrastive Loss는 CNN이 이미지를 유클리드 공간에 나타낼 때, **두개의 이미지**를 비교하여 같은 이미지는 거리를 서로 가깝게, 다른 이미지는 거리를 서로 멀게하는 방식입니다.  
아래와 같이 Contrastive Loss와 Euclidean Distance를 위한 함수를 정의합니다. 

```python
def ContrastiveLoss(y_true,y_pred):
    margin = 1
    return K.mean((1-y_true) * K.square(y_pred) + y_true*K.square(K.maximum(margin-y_pred,0)))
    
def EuclideanDistance(vects):
    x, y = vects
    return K.sqrt(K.sum(K.square(x-y),axis=1, keepdims=True))

def EuclideanDistansOutput(shapes):
    shape1,shape2 = shapes
    return (shape1[0], 1) 
    
```

전체 Siamese Network를 구축하는 함수는 

```python
def BuildSiameseModel():
    front_img = Input(shape = (112,112,3))
    side_img = Input(shape = (112,112,3))

    base_network = EmbeddingNetwork()
    #base_network = BasicCNN()

    ### Model
    vector_a = base_network(front_img)
    vector_b = base_network(side_img)

    distance = Lambda(EuclideanDistance, output_shape= EuclideanDistansOutput)([vector_a,vector_b])
    model = Model([front_img,side_img],distance)

    rms = optimizers.Adam()
    model.compile(loss=ContrastiveLoss,metrics=['acc'],optimizer= rms)

    return model
```




**Triplet Loss**
Triplet Loss는 Anchor 이미지, Positive 이미지, Negative 이미지로 계산됩니다. Anchor 이미지와 Positive 이미지는 동일한(same Identify) 분류를 가지는 이미지며, Negative 이미지는 Anchor 이미지와 동일하지않은 이미지입니다. 
여기서 Triplet loss는 Anchor 이미지와 Positive 이미지의 거리를 최소화하고, 동시에 Anchor 이미지와 Negative 이미지의 거리를 최대화 합니다. 

![Unsplash image 9]({{ site.url }}{{ site.baseurl }}/assets/images/Triplet_Loss.JPG)
**Concept of Triplet Loss**

Triplet Loss를 적용할 때, Contrastive Loss와 동일한 Embedding Network를 사용했고, 
아래와 같이 Loss 함수와 Build 함수만 다르게 구현했습니다. 

```python
def TripletLoss(y_true,y_pred,alpha = 1.0):
    total_lenght = y_pred.shape.as_list()[-1]
    
    anchor = y_pred[:,0:int(total_lenght*1/3)]
    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]
    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]

    # distance between the anchor and the positive
    pos_dist = K.sum(K.square(anchor-positive),axis=1)

    # distance between the anchor and the negative
    neg_dist = K.sum(K.square(anchor-negative),axis=1)

    # compute loss
    basic_loss = pos_dist-neg_dist+alpha
    loss = K.maximum(basic_loss,0.0)
    return loss

def BuildSiameseModel():
    anchor_img = Input(shape = (112,112,3))
    positive_img = Input(shape = (112,112,3))
    negative_img = Input(shape = (112,112,3))

    embedding_network = EmbeddingNetwork()

    ### Model
    vector_anchor = embedding_network(anchor_img)
    vector_positive = embedding_network(positive_img)
    vector_negative = embedding_network(negative_img)

    outputs = [vector_anchor,vector_positive,vector_negative]
    merged_vector = concatenate([vector_anchor,vector_positive,vector_negative])

    model = Model(inputs=[anchor_img,positive_img,negative_img],outputs=merged_vector)
    model.compile(loss=TripletLoss,optimizer= 'adam') 

    return model,embedding_network
```

학습은 BuildSiameseModel에서 model로 진행하면 됩니다. 
Test는 embedding_network를 통해 좌표를 출력하고, 비교하고자하는 이미지의 거리를 L1이나 L2 Distance 함수로 계산하면 됩니다. 

## 후기
Siamese Network에서 큰 영향을 미치는 것 중에 하나는 Training Data set의 순서인 것을 어디선가 본 기억이 있습니다. 실제로도 순서를 바꿔봤을 때 학습이 전혀 진행되지 않는 경우도 있고, 잘 수렴이 되는 경우도 있습니다. 유사한 이미지를 먼저 학습시키면 수렴성에 더 좋다고 한 것 같습니다. 추가로 Quadratic Loss나 다른 Loss를 변형하여 시도해보았지만 결과는 좋아지지 않았습니다. 

이미지 인식은 처음 도전해봤고, 2주의 짧은 기간동안 혼자 진행하기에 조금 더딘부분도 있었습니다. 하지만 공부하면서 큰 도움이 됐고, Few shot learning의 개발 동향에 대해서도 알 수 있는 좋은 경험이었습니다.


## Reference
https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf
http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf
https://arxiv.org/pdf/1503.03832.pdf
